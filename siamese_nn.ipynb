{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Neural Network for predicting PPIs from function annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import click as ck\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Concatenate, Dot, Activation\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "from elembeddings.utils import Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins in training:  5926\n",
      "Training interactions:  152386\n",
      "Validation interactions:  37840\n",
      "Testing interactions:  47284\n"
     ]
    }
   ],
   "source": [
    "org_id = '4932'\n",
    "\n",
    "def load_train_data(data_file):\n",
    "    data = []\n",
    "    proteins = {}\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins:\n",
    "                proteins[id1] = len(proteins)\n",
    "            if id2 not in proteins:\n",
    "                proteins[id2] = len(proteins)\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data, proteins\n",
    "\n",
    "def load_test_data(data_file, proteins):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins or id2 not in proteins:\n",
    "                continue\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data\n",
    "\n",
    "train_data, proteins = load_train_data(f'data/train/{org_id}.protein.links.v11.0.txt')\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "print('Number of proteins in training: ', len(proteins))\n",
    "print('Training interactions: ', len(train_data))\n",
    "print('Validation interactions: ', len(valid_data))\n",
    "print('Testing interactions: ', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functional annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 5275 proteins\n",
      "Total number of distinct functions 8384\n"
     ]
    }
   ],
   "source": [
    "def load_annotations(data_file, proteins, propagate=False):\n",
    "    go = Ontology('data/go.obo')\n",
    "    annots = {}\n",
    "    functions = set()\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split('\\t')\n",
    "            if it[0] not in proteins:\n",
    "                continue\n",
    "            p_id = proteins[it[0]]\n",
    "            if p_id not in annots:\n",
    "                annots[p_id] = set()\n",
    "            annots[p_id].add(it[1])\n",
    "            if propagate and go.has_term(it[1]):\n",
    "                annots[p_id] |= go.get_anchestors(it[1])\n",
    "                functions |= go.get_anchestors(it[1])\n",
    "    functions = list(functions)\n",
    "    return annots, functions\n",
    "\n",
    "# Run this function with propagate=False to use annotations without propagation with ontology structure\n",
    "annotations, functions = load_annotations(f'data/train/{org_id}.annotation.txt', proteins, propagate=True)\n",
    "print('Loaded annotations for', len(annotations), 'proteins')\n",
    "print('Total number of distinct functions', len(functions))\n",
    "functions_ix = {k:i for i, k in enumerate(functions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator object for feeding neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, data, proteins, annotations, train_pairs, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.train_pairs = train_pairs\n",
    "        self.proteins = proteins\n",
    "        self.annotations = annotations\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pos = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            batch_neg = []\n",
    "            for pr1, pr2 in batch_pos:\n",
    "                flag = np.random.choice([True, False])\n",
    "                while True:\n",
    "                    neg = np.random.randint(0, len(self.proteins))\n",
    "                    if flag:\n",
    "                        if (pr1, neg) not in train_pairs:\n",
    "                            batch_neg.append((pr1, neg))\n",
    "                            break\n",
    "                    else:\n",
    "                        if (neg, pr2) not in train_pairs:\n",
    "                            batch_neg.append((neg, pr2))\n",
    "                            break\n",
    "            batch_data = np.array(batch_pos + batch_neg)\n",
    "            labels = np.array([1] * len(batch_pos) + [0] * len(batch_neg))\n",
    "            index = np.arange(len(batch_data))\n",
    "            np.random.shuffle(index)\n",
    "            batch_data = batch_data[index]\n",
    "            labels = labels[index]\n",
    "            p1 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_data)):\n",
    "                if batch_data[i, 0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_data[i, 1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "train_pairs = set(train_data)\n",
    "batch_size = 128\n",
    "train_steps = int(math.ceil(len(train_data) / batch_size))\n",
    "train_generator = Generator(\n",
    "    train_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=train_steps)\n",
    "valid_steps = int(math.ceil(len(valid_data) / batch_size))\n",
    "valid_generator = Generator(\n",
    "    valid_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=valid_steps)\n",
    "test_steps = int(math.ceil(len(test_data) / batch_size))\n",
    "test_generator = Generator(\n",
    "    test_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 8384)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 8384)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 256)          9242368     input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,242,368\n",
      "Trainable params: 9,242,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_model = Sequential()\n",
    "feature_model.add(Dense(1024, input_shape=(len(functions),), activation='relu'))\n",
    "feature_model.add(Dense(512, activation='relu'))\n",
    "feature_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "input1 = Input(shape=(len(functions),))\n",
    "input2 = Input(shape=(len(functions),))\n",
    "feature1 = feature_model(input1)\n",
    "feature2 = feature_model(input2)\n",
    "net = Dot(axes=1)([feature1, feature2])\n",
    "net = Activation('sigmoid')(net)\n",
    "model = Model(inputs=[input1, input2], outputs=net)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1191/1191 [==============================] - 94s 79ms/step - loss: 0.5146 - val_loss: 0.4926\n",
      "Epoch 2/12\n",
      "1191/1191 [==============================] - 101s 85ms/step - loss: 0.4907 - val_loss: 0.4868\n",
      "Epoch 3/12\n",
      "1191/1191 [==============================] - 102s 86ms/step - loss: 0.4802 - val_loss: 0.4792\n",
      "Epoch 4/12\n",
      "1191/1191 [==============================] - 102s 85ms/step - loss: 0.4764 - val_loss: 0.4768\n",
      "Epoch 5/12\n",
      "1191/1191 [==============================] - 99s 83ms/step - loss: 0.4725 - val_loss: 0.4732\n",
      "Epoch 6/12\n",
      "1191/1191 [==============================] - 100s 84ms/step - loss: 0.4712 - val_loss: 0.4701\n",
      "Epoch 7/12\n",
      "1191/1191 [==============================] - 95s 79ms/step - loss: 0.4697 - val_loss: 0.4690\n",
      "Epoch 8/12\n",
      "1191/1191 [==============================] - 96s 81ms/step - loss: 0.4676 - val_loss: 0.4653\n",
      "Epoch 9/12\n",
      "1191/1191 [==============================] - 100s 84ms/step - loss: 0.4650 - val_loss: 0.4655\n",
      "Epoch 10/12\n",
      "1191/1191 [==============================] - 99s 83ms/step - loss: 0.4644 - val_loss: 0.4666\n",
      "Epoch 11/12\n",
      "1191/1191 [==============================] - 98s 83ms/step - loss: 0.4627 - val_loss: 0.4619\n",
      "Epoch 12/12\n",
      "1191/1191 [==============================] - 100s 84ms/step - loss: 0.4624 - val_loss: 0.4638\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-108e4767dd63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     callbacks=[earlystopper,])\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KAUST/CBRC/machine-learning-with-ontologies/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m         verbose=verbose)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m   def predict_generator(self,\n",
      "\u001b[0;32m~/KAUST/CBRC/machine-learning-with-ontologies/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m       raise ValueError('`steps=None` is only valid for a generator'\n\u001b[0m\u001b[1;32m    269\u001b[0m                        \u001b[0;34m' based on the `keras.utils.Sequence` class.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                        \u001b[0;34m' Please specify `steps` or use the'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "earlystopper = EarlyStopping(patience=3)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=[earlystopper,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 9s 26ms/step\n",
      "Test loss: 0.4660752353334616\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate_generator(test_generator, steps=test_steps, verbose=1)\n",
    "print('Test loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction scores for all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test proteins: 5926\n",
      "274356/274356 [==============================] - 4439s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Total number of test proteins:', len(proteins))\n",
    "all_pairs = []\n",
    "for i in range(len(proteins)):\n",
    "    for j in range(len(proteins)):\n",
    "        all_pairs.append((i, j))\n",
    "\n",
    "batch_size = 128\n",
    "class SimpleGenerator(object):\n",
    "\n",
    "    def __init__(self, data, annotations, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.annotations = annotations\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pairs = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            p1 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_pairs)):\n",
    "                if batch_pairs[i][0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_pairs[i][1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            labels = np.zeros((len(batch_pairs), 1), dtype=np.float32)\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "all_steps = int(math.ceil(len(all_pairs) / batch_size))\n",
    "all_generator = SimpleGenerator(\n",
    "    all_pairs, annotations, functions_ix,\n",
    "    batch_size=batch_size, steps=all_steps)\n",
    "predictions = model.predict_generator(all_generator, steps=all_steps, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.08 0.50 543.56 0.91\n",
      "0.19 0.72 491.56 0.92\n"
     ]
    }
   ],
   "source": [
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "sim = predictions.reshape(len(proteins), len(proteins))\n",
    "\n",
    "trlabels = np.ones((len(proteins), len(proteins)), dtype=np.int32)\n",
    "for c, d in train_data:\n",
    "    trlabels[c, d] = 0\n",
    "for c, d in valid_data:\n",
    "    trlabels[c, d] = 0\n",
    "\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "n = len(test_data)\n",
    "labels = np.zeros((len(proteins), len(proteins)), dtype=np.int32) \n",
    "ranks = {}\n",
    "franks = {}\n",
    "with ck.progressbar(test_data) as prog_data:\n",
    "    for c, d in prog_data:\n",
    "        labels[c, d] = 1\n",
    "        index = rankdata(-sim[c, :], method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            top10 += 1\n",
    "        if rank <= 100:\n",
    "            top100 += 1\n",
    "        mean_rank += rank\n",
    "        if rank not in ranks:\n",
    "            ranks[rank] = 0\n",
    "        ranks[rank] += 1\n",
    "\n",
    "        # Filtered rank\n",
    "        fil = sim[c, :] * (labels[c, :] | trlabels[c, :])\n",
    "        index = rankdata(-fil, method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            ftop10 += 1\n",
    "        if rank <= 100:\n",
    "            ftop100 += 1\n",
    "        fmean_rank += rank\n",
    "        if rank not in franks:\n",
    "            franks[rank] = 0\n",
    "        franks[rank] += 1\n",
    "\n",
    "    print()\n",
    "    top10 /= n\n",
    "    top100 /= n\n",
    "    mean_rank /= n\n",
    "    ftop10 /= n\n",
    "    ftop100 /= n\n",
    "    fmean_rank /= n\n",
    "\n",
    "    rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "    frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "    print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "    print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
